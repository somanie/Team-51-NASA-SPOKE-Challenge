{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8649daf9-c0b5-4a67-adca-e4e41b7c7898",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Download and Filter NASA GeneLab Omics and Non-Omics Datasets\n",
    "\n",
    "This notebook automates the retrieval and preâ€‘processing of omics datasets from the NASA GeneLab Open Science Data Repository (OSDR) using the `genelab_utils` package. It supports both incremental and full updates, applies preâ€‘filters to reduce file size, and writes a manifest of downloaded files.\n",
    "\n",
    "Author: Chisom Aniekwensi (sommaniekwensi@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d748a11-d295-49b2-95ba-e188fa703c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and packages\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcd6ad7-cb76-4cdb-aacd-d470daf87ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"nasa_kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317c674c-305f-4076-ac16-ea33c7135aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File type patterns\n",
    "OMICS_PATTERNS = ['rna-seq', 'rna_seq', 'transcript', 'microarray', 'methylation', \n",
    "                 'proteom', 'metabolom', 'gene', 'seq', 'diff_expr', 'deg']\n",
    "NON_OMICS_PATTERNS = ['imaging', 'microscopy', 'tomography', 'pressure', 'temperature',\n",
    "                      'tonometry', 'sensor', 'ultrasonography', 'biotelemetry', 'mri']\n",
    "\n",
    "# GLDS to OSD mapping\n",
    "GLDS_TO_OSD = {\n",
    "    'GLDS-100': 'OSD-100', 'GLDS-162': 'OSD-162', 'GLDS-194': 'OSD-194',\n",
    "    'GLDS-203': 'OSD-203', 'GLDS-255': 'OSD-255', 'GLDS-397': 'OSD-397',\n",
    "    'GLDS-87': 'OSD-87'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61193bbb-813f-45ee-8f5c-0176bfedd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract OSDR dataset ID from filename.\n",
    "def extract_dataset_id(filename: str) -> Optional[str]:\n",
    "    filename_lower = str(filename).lower()\n",
    "    \n",
    "    # OSD pattern\n",
    "    osd_match = re.search(r'(osd-\\d+)', filename_lower)\n",
    "    if osd_match:\n",
    "        osd_num = re.search(r'osd-(\\d+)', osd_match.group(1)).group(1)\n",
    "        return f\"OSD-{osd_num}\"\n",
    "    \n",
    "    # s_OSD pattern\n",
    "    s_osd_match = re.search(r's_osd-(\\d+)', filename_lower)\n",
    "    if s_osd_match:\n",
    "        return f\"OSD-{s_osd_match.group(1)}\"\n",
    "    \n",
    "    # ALSDA pattern\n",
    "    alsda_match = re.search(r'(alsda[-_]\\d+)', filename_lower)\n",
    "    if alsda_match:\n",
    "        alsda_num = re.search(r'alsda[-_](\\d+)', alsda_match.group(1)).group(1)\n",
    "        return f\"ALSDA-{alsda_num}\"\n",
    "    \n",
    "    # GLDS pattern and mapping\n",
    "    glds_match = re.search(r'(glds-\\d+)', filename_lower)\n",
    "    if glds_match:\n",
    "        glds_num = re.search(r'glds-(\\d+)', glds_match.group(1)).group(1)\n",
    "        glds_id = f\"GLDS-{glds_num}\"\n",
    "        return GLDS_TO_OSD.get(glds_id)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96cbc23-b24f-4abb-b4c4-f1246c94506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect if file is omics, non-omics, or metadata\n",
    "\n",
    "def detect_file_type(file_path: str) -> tuple:\n",
    "    filename = str(file_path).lower()\n",
    "    dataset_id = extract_dataset_id(filename)\n",
    "    \n",
    "    # Check if this is a metadata file (s_OSD)\n",
    "    if 's_osd' in filename:\n",
    "        return ('metadata', dataset_id, 's_file_pattern')\n",
    "    \n",
    "    # Check for omics patterns\n",
    "    if any(pattern in filename for pattern in OMICS_PATTERNS):\n",
    "        return ('omics', dataset_id, 'omics_pattern')\n",
    "    \n",
    "    # Check for non-omics patterns\n",
    "    if any(pattern in filename for pattern in NON_OMICS_PATTERNS):\n",
    "        return ('non_omics', dataset_id, 'non_omics_pattern')\n",
    "    \n",
    "    # ALSDA files are typically omics\n",
    "    if 'alsda' in filename:\n",
    "        return ('omics', dataset_id, 'alsda_file')\n",
    "    \n",
    "    return ('unknown', dataset_id, 'fallback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d040d3f-b2fe-4bd9-91bf-d340f3e31c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch basic metadata for an OSDR dataset\n",
    "\n",
    "def get_osdr_metadata(accession: str) -> Dict:\n",
    "    url = f\"https://visualization.osdr.nasa.gov/biodata/api/v2/dataset/{accession}/?format=json\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=60)\n",
    "        if response.status_code != 200:\n",
    "            logger.warning(f\"Failed to get metadata for {accession}: Status {response.status_code}\")\n",
    "            return {\"identifier\": accession, \"error\": f\"HTTP {response.status_code}\"}\n",
    "        \n",
    "        data = response.json()\n",
    "        ds = data.get(accession, {})\n",
    "        meta = ds.get(\"metadata\", {})\n",
    "        \n",
    "        # Create standardized metadata structure\n",
    "        metadata = {\n",
    "            \"identifier\": accession,\n",
    "            \"study_title\": meta.get(\"project title\", \"\"),\n",
    "            \"organism\": meta.get(\"organism\", \"\"),\n",
    "            \"mission\": meta.get(\"mission\", {}).get(\"name\", \"\")\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching metadata for {accession}: {str(e)}\")\n",
    "        return {\"identifier\": accession, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e08ce8-d60f-4668-acd2-600cfe60126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to safely read a file into a DataFrame, handling various formats and issues\n",
    "\n",
    "def safe_read_file(file_path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        if file_path.suffix.lower() in ['.csv', '.txt', '.tsv']:\n",
    "            # First try with standard parameters\n",
    "            try:\n",
    "                return pd.read_csv(file_path, on_bad_lines='skip', low_memory=False)\n",
    "            except Exception as e1:\n",
    "                # If that fails, try with more flexible parameters\n",
    "                try:\n",
    "                    return pd.read_csv(file_path, sep=None, engine='python', on_bad_lines='skip')\n",
    "                except Exception as e2:\n",
    "                    # Last resort: try to read as text and parse manually\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    # Create a simple DataFrame with the text content\n",
    "                    return pd.DataFrame({'content': lines})\n",
    "                    \n",
    "        elif file_path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "            # Try to read Excel files\n",
    "            try:\n",
    "                return pd.read_excel(file_path)\n",
    "            except Exception as e:\n",
    "                # If Excel reading fails, try to read as binary and create placeholder\n",
    "                return pd.DataFrame({'filename': [file_path.name], \n",
    "                                    'error': [f'Could not parse Excel: {str(e)}']})\n",
    "        else:\n",
    "            # For unsupported formats, create a simple DataFrame\n",
    "            return pd.DataFrame({'filename': [file_path.name], \n",
    "                                'format': [file_path.suffix]})\n",
    "    except Exception as e:\n",
    "        # If all else fails, return an empty DataFrame with error info\n",
    "        return pd.DataFrame({'filename': [file_path.name], \n",
    "                            'error': [f'File reading error: {str(e)}']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a929e68a-8567-4219-8cda-4aedc0245032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to DataFrame safely, handling MultiIndex and other issues\n",
    "\n",
    "def add_metadata(df: pd.DataFrame, metadata_dict: Dict) -> pd.DataFrame:\n",
    "    try:\n",
    "        # If DataFrame is empty, return it with metadata as first row\n",
    "        if df.empty:\n",
    "            return pd.DataFrame([metadata_dict])\n",
    "        \n",
    "        # Check if df has MultiIndex\n",
    "        if isinstance(df.index, pd.MultiIndex):\n",
    "            # Reset index to avoid MultiIndex issues\n",
    "            df_reset = df.reset_index()\n",
    "            \n",
    "            # Add metadata columns one by one\n",
    "            for key, value in metadata_dict.items():\n",
    "                df_reset[key] = value\n",
    "                \n",
    "            return df_reset\n",
    "        else:\n",
    "            # For regular DataFrames, create a metadata DataFrame with same length\n",
    "            metadata_df = pd.DataFrame([metadata_dict] * len(df))\n",
    "            \n",
    "            # Add a temporary index column to both DataFrames\n",
    "            df['_temp_idx'] = range(len(df))\n",
    "            metadata_df['_temp_idx'] = range(len(df))\n",
    "            \n",
    "            # Merge on the temp index\n",
    "            result = pd.merge(df, metadata_df, on='_temp_idx')\n",
    "            \n",
    "            # Remove the temporary index\n",
    "            result = result.drop('_temp_idx', axis=1)\n",
    "            \n",
    "            return result\n",
    "    except Exception as e:\n",
    "        # If adding metadata fails, create a new DataFrame with original data and metadata\n",
    "        logger.warning(f\"Error adding metadata: {str(e)}. Creating new DataFrame.\")\n",
    "        \n",
    "        # Create a simple summary of the original DataFrame\n",
    "        summary = {\n",
    "            'original_columns': list(df.columns),\n",
    "            'row_count': len(df),\n",
    "            'issue': str(e)\n",
    "        }\n",
    "        \n",
    "        # Combine with metadata\n",
    "        combined_dict = {**metadata_dict, **summary}\n",
    "        \n",
    "        # First row is metadata/summary, rest is empty\n",
    "        result = pd.DataFrame([combined_dict])\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d521d1d9-c2a1-45f2-a290-2c36deb1539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process a single file and save to output directory, handling various edge cases\n",
    "\n",
    "def process_file(file_path: Path, output_dir: Path, metadata: Dict = None) -> Dict:\n",
    "    # Detect file type\n",
    "    file_type, dataset_id, detection_method = detect_file_type(str(file_path))\n",
    "    \n",
    "    # Create basic result structure\n",
    "    result = {\n",
    "        \"file_path\": str(file_path),\n",
    "        \"file_type\": file_type,\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"detection_method\": detection_method,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Read the file safely\n",
    "        df = safe_read_file(file_path)\n",
    "        \n",
    "        # Create metadata dictionary\n",
    "        metadata_dict = {\n",
    "            \"file_source\": str(file_path.name),\n",
    "            \"file_type\": file_type,\n",
    "            \"dataset_id\": dataset_id,\n",
    "        }\n",
    "        \n",
    "        # Add OSDR metadata if available\n",
    "        if metadata and dataset_id in metadata:\n",
    "            study_meta = metadata[dataset_id]\n",
    "            metadata_dict[\"study_title\"] = study_meta.get(\"study_title\", \"\")\n",
    "            metadata_dict[\"organism\"] = study_meta.get(\"organism\", \"\")\n",
    "            metadata_dict[\"mission\"] = study_meta.get(\"mission\", \"\")\n",
    "        \n",
    "        # Add metadata to the DataFrame\n",
    "        result_df = add_metadata(df, metadata_dict)\n",
    "        \n",
    "        # Create output path\n",
    "        output_subdir = output_dir / file_type\n",
    "        output_subdir.mkdir(exist_ok=True, parents=True)\n",
    "        output_filename = f\"{file_path.stem}_processed.csv\"\n",
    "        output_path = output_subdir / output_filename\n",
    "        \n",
    "        # Save processed file\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Update result with success info\n",
    "        result.update({\n",
    "            \"output_path\": str(output_path),\n",
    "            \"rows\": len(df),\n",
    "            \"columns\": len(df.columns),\n",
    "            \"processed\": True\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "        result[\"error\"] = str(e)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12fdc1d5-1b02-455a-a092-17ee415a03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process NASA OSDR data files with robust error handling\n",
    "\n",
    "def process_nasa_data(data_dir=None, output_dir=None, max_files=None, parallel=True):\n",
    "    # Set up directories\n",
    "    data_dir = Path(data_dir) if data_dir else Path.cwd() / \"data\"\n",
    "    output_dir = Path(output_dir) if output_dir else Path.cwd() / \"processed\"\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    logger.info(f\"Processing NASA OSDR data from {data_dir}\")\n",
    "    \n",
    "    # Scan for files\n",
    "    extensions = ['.csv', '.txt', '.xlsx', '.xls', '.tsv']\n",
    "    all_files = []\n",
    "    for ext in extensions:\n",
    "        all_files.extend(list(data_dir.glob(f\"**/*{ext}\")))\n",
    "    \n",
    "    # Sort files (metadata first, then data files)\n",
    "    all_files.sort(key=lambda f: 0 if 's_osd' in str(f).lower() else 1)\n",
    "    \n",
    "    # Limit number of files if requested\n",
    "    if max_files:\n",
    "        all_files = all_files[:max_files]\n",
    "    \n",
    "    if not all_files:\n",
    "        logger.warning(f\"No files found in {data_dir}\")\n",
    "        return {\"status\": \"warning\", \"message\": \"No files found\"}\n",
    "    \n",
    "    logger.info(f\"Found {len(all_files)} files to process\")\n",
    "    \n",
    "    # Extract dataset IDs and fetch metadata\n",
    "    dataset_ids = set()\n",
    "    for file in all_files:\n",
    "        dataset_id = extract_dataset_id(str(file))\n",
    "        if dataset_id:\n",
    "            dataset_ids.add(dataset_id)\n",
    "    \n",
    "    logger.info(f\"Found {len(dataset_ids)} unique dataset IDs\")\n",
    "    \n",
    "    # Fetch metadata for all datasets\n",
    "    metadata = {}\n",
    "    for dataset_id in dataset_ids:\n",
    "        metadata[dataset_id] = get_osdr_metadata(dataset_id)\n",
    "    \n",
    "    # Create subdirectories for file types\n",
    "    for dir_name in ['metadata', 'omics', 'non_omics', 'unknown', 'error']:\n",
    "        (output_dir / dir_name).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Process files\n",
    "    results = []\n",
    "    \n",
    "    # Parallel processing\n",
    "    if parallel:\n",
    "        with ThreadPoolExecutor(max_workers=min(os.cpu_count() or 4, 6)) as executor:\n",
    "            futures = [executor.submit(process_file, file, output_dir, metadata) for file in all_files]\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in thread: {str(e)}\")\n",
    "                    results.append({\"file_path\": \"unknown\", \"file_type\": \"error\", \"error\": str(e)})\n",
    "    # Sequential processing (for easier debugging)\n",
    "    else:\n",
    "        for file in all_files:\n",
    "            try:\n",
    "                result = process_file(file, output_dir, metadata)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file: {str(e)}\")\n",
    "                results.append({\"file_path\": str(file), \"file_type\": \"error\", \"error\": str(e)})\n",
    "    \n",
    "    # Create manifest DataFrame\n",
    "    manifest_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add success/error counts\n",
    "    success_count = sum(1 for result in results if not result.get('error'))\n",
    "    error_count = sum(1 for result in results if result.get('error'))\n",
    "    logger.info(f\"Successfully processed {success_count} files. Errors: {error_count}\")\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_path = output_dir / \"manifest.csv\"\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        \"total_files\": len(results),\n",
    "        \"success_count\": success_count,\n",
    "        \"error_count\": error_count,\n",
    "        \"dataset_ids\": list(dataset_ids),\n",
    "    }\n",
    "    \n",
    "    # Add file type counts if possible\n",
    "    if 'file_type' in manifest_df.columns:\n",
    "        file_types = manifest_df['file_type'].value_counts().to_dict()\n",
    "        summary[\"file_type_counts\"] = file_types\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = output_dir / \"summary.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Processing complete. Results saved to {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"manifest_df\": manifest_df,\n",
    "        \"summary\": summary,\n",
    "        \"manifest_path\": str(manifest_path),\n",
    "        \"summary_path\": str(summary_path),\n",
    "        \"success_rate\": f\"{success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b1c0a72-0470-4084-aab1-035665f83b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 14:45:02,763 - INFO - Processing NASA OSDR data from C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\NASA_KnowHax_2025\\data\n",
      "2025-05-06 14:45:02,921 - INFO - Found 147 files to process\n",
      "2025-05-06 14:45:02,921 - INFO - Found 17 unique dataset IDs\n",
      "2025-05-06 14:45:15,901 - WARNING - Failed to get metadata for OSD-758: Status 422\n",
      "2025-05-06 14:45:16,977 - WARNING - Failed to get metadata for OSD-759: Status 422\n",
      "2025-05-06 14:45:53,802 - WARNING - Error adding metadata: Length of values (2) does not match length of index (15). Creating new DataFrame.\n",
      "2025-05-06 14:45:54,140 - WARNING - Error adding metadata: Length of values (2) does not match length of index (12). Creating new DataFrame.\n",
      "2025-05-06 14:45:54,179 - WARNING - Error adding metadata: Length of values (2) does not match length of index (22). Creating new DataFrame.\n",
      "2025-05-06 14:45:54,205 - WARNING - Error adding metadata: Length of values (2) does not match length of index (11). Creating new DataFrame.\n",
      "2025-05-06 14:45:54,260 - WARNING - Error adding metadata: Length of values (2) does not match length of index (11). Creating new DataFrame.\n",
      "2025-05-06 14:45:54,298 - WARNING - Error adding metadata: Length of values (2) does not match length of index (12). Creating new DataFrame.\n",
      "2025-05-06 14:45:54,371 - WARNING - Error adding metadata: Length of values (2) does not match length of index (23). Creating new DataFrame.\n",
      "2025-05-06 14:45:57,563 - WARNING - Error adding metadata: Length of values (2) does not match length of index (11). Creating new DataFrame.\n",
      "2025-05-06 14:45:57,716 - WARNING - Error adding metadata: Length of values (2) does not match length of index (12). Creating new DataFrame.\n",
      "2025-05-06 14:45:57,907 - WARNING - Error adding metadata: Length of values (2) does not match length of index (23). Creating new DataFrame.\n",
      "C:\\Users\\QUCOON\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "2025-05-06 14:48:17,946 - INFO - Successfully processed 147 files. Errors: 0\n",
      "2025-05-06 14:48:17,972 - INFO - Processing complete. Results saved to C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\NASA_KnowHax_2025\\processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_type</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>detection_method</th>\n",
       "      <th>output_path</th>\n",
       "      <th>rows</th>\n",
       "      <th>columns</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>OSD-100</td>\n",
       "      <td>s_file_pattern</td>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>OSD-162</td>\n",
       "      <td>s_file_pattern</td>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>21</td>\n",
       "      <td>89</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>OSD-194</td>\n",
       "      <td>s_file_pattern</td>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>13</td>\n",
       "      <td>73</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>OSD-203</td>\n",
       "      <td>s_file_pattern</td>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>OSD-255</td>\n",
       "      <td>s_file_pattern</td>\n",
       "      <td>C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...</td>\n",
       "      <td>16</td>\n",
       "      <td>73</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path file_type dataset_id  \\\n",
       "0  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...  metadata    OSD-100   \n",
       "1  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...  metadata    OSD-162   \n",
       "2  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...  metadata    OSD-194   \n",
       "3  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...  metadata    OSD-203   \n",
       "4  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...  metadata    OSD-255   \n",
       "\n",
       "  detection_method                                        output_path  rows  \\\n",
       "0   s_file_pattern  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...    12   \n",
       "1   s_file_pattern  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...    21   \n",
       "2   s_file_pattern  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...    13   \n",
       "3   s_file_pattern  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...    59   \n",
       "4   s_file_pattern  C:\\Users\\QUCOON\\Documents\\Chisom_Personal_Doc\\...    16   \n",
       "\n",
       "   columns  processed  \n",
       "0       57       True  \n",
       "1       89       True  \n",
       "2       73       True  \n",
       "3       64       True  \n",
       "4       73       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 147/147 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Print result\n",
    "result = process_nasa_data(\n",
    "    data_dir=\"C:/Users/QUCOON/Documents/Chisom_Personal_Doc/NASA_KnowHax_2025/data\", \n",
    "    output_dir=\"C:/Users/QUCOON/Documents/Chisom_Personal_Doc/NASA_KnowHax_2025/processed\",\n",
    "    parallel=True,  # Set to False for debugging\n",
    "    max_files=None  # Set to None to process all files\n",
    ")\n",
    "display(result[\"manifest_df\"].head())\n",
    "print(f\"Success rate: {result['success_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da500264-1a10-4db1-b037-f5532e5de564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f615d4-4e84-4a85-91a3-f7d0295149de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbe423-d608-4465-8b48-8539d9f363b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530ac6c-83ac-4193-b323-afdc3e955a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
